{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMC+5toMpd4ED53lAMgVCyy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenminhvuinfo/250505-mern/blob/main/Simple_VQA_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "ocIo5PvN8CHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, DataCollatorWithPadding, AutoModel, AutoModelForCausalLM, AutoImageProcessor\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from datasets import load_dataset, Dataset"
      ],
      "metadata": {
        "id": "GmpCrbzn8-qm"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-uVjiHr69CBM",
        "outputId": "863a49a8-9740-4f99-b187-96424a7968d6"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_ENCODER_MODEL = 'google/vit-base-patch16-224'\n",
        "TEXT_ENCODER_MODEL = 'distilbert/distilbert-base-cased-distilled-squad'\n",
        "DECODER_MODEL = 'gpt2'"
      ],
      "metadata": {
        "id": "hGjhNR8l98ej"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_tokenizer = AutoTokenizer.from_pretrained(DECODER_MODEL)\n",
        "decoder_tokenizer.pad_token = decoder_tokenizer.eos_token\n",
        "decoder_tokenizer.padding_side = \"left\""
      ],
      "metadata": {
        "id": "xCROTeNc-F7R"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building the model"
      ],
      "metadata": {
        "id": "X7ctNTkv8rSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalVQAModel(nn.Module):\n",
        "\n",
        "    def __init__(self, text_encoder_model: str, image_encoder_model: str, decoder_model: str):\n",
        "        super(MultimodalVQAModel, self).__init__()\n",
        "\n",
        "        # Load pre-trained models\n",
        "        self.text_encoder = AutoModel.from_pretrained(text_encoder_model).to(device)\n",
        "        self.image_encoder = AutoModel.from_pretrained(image_encoder_model).to(device)\n",
        "        self.decoder = AutoModelForCausalLM.from_pretrained(\n",
        "            decoder_model,\n",
        "            add_cross_attention=True,\n",
        "            tie_word_embeddings=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Linear layers to project text and image features to the decoder's hidden size\n",
        "        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
        "        self.image_proj = nn.Linear(self.image_encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
        "\n",
        "    def forward(self, input_text, input_image, decoder_input_ids, attention_mask, labels=None):\n",
        "        # Encode text\n",
        "        text_features = self.encode_text(input_text, attention_mask)\n",
        "        # Encode image\n",
        "        image_features = self.encode_image(input_image)\n",
        "\n",
        "        combined_features = (text_features + image_features) / 2\n",
        "\n",
        "        # Decoding\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            labels=labels,\n",
        "            encoder_hidden_states=combined_features.unsqueeze(1),\n",
        "        )\n",
        "        return decoder_outputs\n",
        "\n",
        "    def encode_text(self, input_text, attention_mask):\n",
        "        text_outputs = self.text_encoder(input_text, attention_mask=attention_mask)\n",
        "        text_features = text_outputs.last_hidden_state.mean(dim=1)\n",
        "        return self.text_proj(text_features)\n",
        "\n",
        "    def encode_image(self, input_image):\n",
        "        image_outputs = self.image_encoder(input_image)\n",
        "        image_features = image_outputs.pooler_output\n",
        "        return self.image_proj(image_features)"
      ],
      "metadata": {
        "id": "NKgoUx159Bjo"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Processing"
      ],
      "metadata": {
        "id": "XHW3nKt88rVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HuggingFaceM4/A-OKVQA\")\n",
        "train_ds = dataset['train']\n",
        "val_ds = dataset['validation']\n",
        "test_ds = dataset['test']\n"
      ],
      "metadata": {
        "id": "gpzP6ADt-gQP"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
        "train_ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8jtC1-e_FLU",
        "outputId": "836267d6-da49-4344-f2d1-e04ff4812ed4"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 17056, Val: 1145, Test: 6702\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>,\n",
              " 'question_id': '22MexNkBPpdZGX6sxbxVBH',\n",
              " 'question': 'What is the man by the bags awaiting?',\n",
              " 'choices': ['skateboarder', 'train', 'delivery', 'cab'],\n",
              " 'correct_choice_idx': 3,\n",
              " 'direct_answers': \"['ride', 'ride', 'bus', 'taxi', 'travelling', 'traffic', 'taxi', 'cab', 'cab', 'his ride']\",\n",
              " 'difficult_direct_answer': False,\n",
              " 'rationales': ['A train would not be on the street, he would not have luggage waiting for a delivery, and the skateboarder is there and not paying attention to him so a cab is the only possible answer.',\n",
              "  'He has bags as if he is going someone, and he is on a road waiting for vehicle that can only be moved on the road and is big enough to hold the bags.',\n",
              "  'He looks to be waiting for a paid ride to pick him up.']}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoder_tokenizer = AutoTokenizer.from_pretrained(TEXT_ENCODER_MODEL)\n",
        "image_feature_extractor = AutoImageProcessor.from_pretrained(IMAGE_ENCODER_MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1hoJmPp_IhX",
        "outputId": "1db9716d-7b97-4ed9-8c40-b7eda1e3df5e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def data_collator(batch):\n",
        "    # Text inputs\n",
        "    text_inputs = [sample['question'] for sample in batch]\n",
        "    text_tensors = text_encoder_tokenizer(text_inputs, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Image inputs\n",
        "    images = [sample['image'].convert('RGB') for sample in batch]\n",
        "    image_inputs = image_feature_extractor(images, return_tensors=\"pt\")\n",
        "    image_tensors = image_inputs['pixel_values']\n",
        "\n",
        "    # âœ… FIX: Parse string thÃ nh list trÆ°á»›c\n",
        "    target_inputs = []\n",
        "    for sample in batch:\n",
        "        # Parse string thÃ nh list\n",
        "        answers = ast.literal_eval(sample['direct_answers'])\n",
        "        # Láº¥y answer Ä‘áº§u tiÃªn\n",
        "        answer = answers[0]\n",
        "        target_inputs.append(f\"<|endoftext|>{answer}<|endoftext|>\")\n",
        "\n",
        "    target_tensors = decoder_tokenizer(target_inputs, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Labels\n",
        "    labels = target_tensors[\"input_ids\"].clone()\n",
        "    labels = torch.where((labels == decoder_tokenizer.pad_token_id), -100, labels)\n",
        "    labels[:, -1] = decoder_tokenizer.eos_token_id\n",
        "\n",
        "    return {\n",
        "        \"input_text\": text_tensors[\"input_ids\"],\n",
        "        \"attention_mask\": text_tensors[\"attention_mask\"],\n",
        "        \"input_image\": image_tensors,\n",
        "        \"decoder_input_ids\": target_tensors[\"input_ids\"],\n",
        "        \"labels\": labels\n",
        "    }"
      ],
      "metadata": {
        "id": "j_a9G-Dq_Ip4"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data_collator vá»›i fix má»›i\n",
        "batch = data_collator([train_ds[0], train_ds[1]])\n",
        "\n",
        "print(\"Sample 0:\")\n",
        "print(f\"  Question: {train_ds[0]['question']}\")\n",
        "print(f\"  Direct answers (raw): {train_ds[0]['direct_answers']}\")\n",
        "print(f\"  Decoder input: {decoder_tokenizer.decode(batch['decoder_input_ids'][0])}\")\n",
        "print(f\"  Labels: {decoder_tokenizer.decode([t for t in batch['labels'][0] if t != -100])}\")\n",
        "\n",
        "print(\"\\nSample 1:\")\n",
        "print(f\"  Question: {train_ds[1]['question']}\")\n",
        "print(f\"  Direct answers (raw): {train_ds[1]['direct_answers']}\")\n",
        "print(f\"  Decoder input: {decoder_tokenizer.decode(batch['decoder_input_ids'][1])}\")\n",
        "print(f\"  Labels: {decoder_tokenizer.decode([t for t in batch['labels'][1] if t != -100])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABFX7m7TQfxX",
        "outputId": "33c90f76-e9e4-45dd-8e8c-b352743da0e7"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0:\n",
            "  Question: What is the man by the bags awaiting?\n",
            "  Direct answers (raw): ['ride', 'ride', 'bus', 'taxi', 'travelling', 'traffic', 'taxi', 'cab', 'cab', 'his ride']\n",
            "  Decoder input: <|endoftext|>ride<|endoftext|>\n",
            "  Labels: ride<|endoftext|>\n",
            "\n",
            "Sample 1:\n",
            "  Question: Where does this man eat pizza?\n",
            "  Direct answers (raw): ['work', 'office', 'work', 'work', 'at work', 'desk', 'at desk', 'office', 'work desk', 'office']\n",
            "  Decoder input: <|endoftext|>work<|endoftext|>\n",
            "  Labels: work<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator([train_ds[0], train_ds[1]])\n",
        "print(\"Input text shape:\", batch['input_text'].shape)\n",
        "print(\"Input image shape:\", batch['input_image'].shape)\n",
        "print(\"Labels shape:\", batch['labels'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSsge91l_hQA",
        "outputId": "b294028c-42e0-4564-8fbc-853ae7d1e4f2"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text shape: torch.Size([2, 11])\n",
            "Input image shape: torch.Size([2, 3, 224, 224])\n",
            "Labels shape: torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the multimodal VQA model\n"
      ],
      "metadata": {
        "id": "myx807PO8rYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./aokvqa_output\",\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_safetensors=False,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=128,\n",
        "    logging_steps=10,\n",
        "    report_to='none',\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=2e-5,\n",
        "    lr_scheduler_type='cosine',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_num_workers=4,\n",
        ")"
      ],
      "metadata": {
        "id": "EhvhGDOS_lS4"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalVQAModel(\n",
        "    TEXT_ENCODER_MODEL,\n",
        "    IMAGE_ENCODER_MODEL,\n",
        "    DECODER_MODEL\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlLR5HeZ_reM",
        "outputId": "7a642616-b2b5-4428-abe5-5d3f71650ce7"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq_COKOS_xA5",
        "outputId": "768a34fc-6761-46d3-a403-ec1c35234549"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 305174784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "MJKZEEyxAYRf"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "AN1B1KkzAaUy",
        "outputId": "403dc3c5-ec72-4365-db81-57aa685d52e0"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1599' max='1599' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1599/1599 11:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.170500</td>\n",
              "      <td>2.912757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.615200</td>\n",
              "      <td>2.739261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.589700</td>\n",
              "      <td>2.697063</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1599, training_loss=3.1480240288043184, metrics={'train_runtime': 694.1198, 'train_samples_per_second': 73.716, 'train_steps_per_second': 2.304, 'total_flos': 0.0, 'train_loss': 3.1480240288043184, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"aokvqa_model.pt\")\n",
        "print(\"âœ… Model saved as aokvqa_model.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfwtHmZgDNMU",
        "outputId": "0c6f1e4b-83ae-4641-8116-10c00892e2dd"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model saved as aokvqa_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST\n"
      ],
      "metadata": {
        "id": "JqSoKhIYDU9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test vá»›i sample tá»« train_ds\n",
        "test_model_train = lambda idx: test_model_from_dataset(idx, train_ds)\n",
        "\n",
        "def test_model_from_dataset(idx, dataset):\n",
        "    sample = dataset[idx]\n",
        "\n",
        "    plt.imshow(sample['image'])\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Q: {sample['question']}\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Ground Truth: {sample['direct_answers']}\")\n",
        "\n",
        "    batch = data_collator([sample])\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(batch[\"input_text\"], batch[\"attention_mask\"])\n",
        "        image_features = model.encode_image(batch[\"input_image\"])\n",
        "        combined_features = (text_features + image_features) / 2\n",
        "\n",
        "        attention_mask = torch.ones((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "        generated_ids = model.decoder.generate(\n",
        "            encoder_hidden_states=combined_features.unsqueeze(1),\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=20,\n",
        "            num_beams=3,\n",
        "            pad_token_id=decoder_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        prediction = decoder_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        print(f\"Prediction: {prediction}\")\n",
        "\n",
        "# Test 10 vÃ­ dá»¥ tá»« táº­p train\n",
        "for i in range(10):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"EXAMPLE {i+1}\")\n",
        "    print('='*50)\n",
        "    test_model_from_dataset(i, train_ds)"
      ],
      "metadata": {
        "id": "QbVnu2eCDgnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== KIá»‚M TRA QUAN TRá»ŒNG NHáº¤T =====\n",
        "print(\"=\"*80)\n",
        "print(\"ðŸ” KIá»‚M TRA: Model Ä‘Ã£ Ä‘Æ°á»£c train chÆ°a?\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Kiá»ƒm tra 1: CÃ³ file checkpoint khÃ´ng?\n",
        "import os\n",
        "checkpoint_dir = \"checkpoints\"  # Thay báº±ng folder cá»§a báº¡n\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth') or f.endswith('.pt')]\n",
        "    print(f\"âœ… TÃ¬m tháº¥y {len(checkpoints)} checkpoints: {checkpoints}\")\n",
        "else:\n",
        "    print(f\"âŒ KHÃ”NG TÃŒM THáº¤Y FOLDER CHECKPOINT!\")\n",
        "    print(\"   => Model Ä‘ang dÃ¹ng RANDOM WEIGHTS - ChÆ°a train!\")\n",
        "\n",
        "# Kiá»ƒm tra 2: Model cÃ³ á»Ÿ evaluation mode khÃ´ng?\n",
        "print(f\"\\nModel training mode: {model.training}\")\n",
        "if model.training:\n",
        "    print(\"âš ï¸ WARNING: Model Ä‘ang á»Ÿ TRAINING mode, pháº£i chuyá»ƒn sang eval!\")\n",
        "    model.eval()\n",
        "\n",
        "# Kiá»ƒm tra 3: Decoder cÃ³ Ä‘Æ°á»£c train khÃ´ng?\n",
        "decoder_trainable_params = sum(p.numel() for p in model.decoder.parameters() if p.requires_grad)\n",
        "decoder_total_params = sum(p.numel() for p in model.decoder.parameters())\n",
        "print(f\"\\nDecoder trainable params: {decoder_trainable_params:,} / {decoder_total_params:,}\")\n",
        "\n",
        "if decoder_trainable_params == 0:\n",
        "    print(\"âŒ DECODER Bá»Š FREEZE - KhÃ´ng thá»ƒ train Ä‘Æ°á»£c!\")\n",
        "    print(\"   => Pháº£i unfreeze decoder trÆ°á»›c khi train\")\n",
        "\n",
        "# ===== NGUYÃŠN NHÃ‚N: Báº¡n chÆ°a train model =====\n",
        "# âž¡ï¸ GIáº¢I PHÃP: Train model trÆ°á»›c khi test!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcUxhfXrVQMM",
        "outputId": "73423947-2e14-462b-ce39-302a1c67bd34"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ” KIá»‚M TRA: Model Ä‘Ã£ Ä‘Æ°á»£c train chÆ°a?\n",
            "================================================================================\n",
            "âŒ KHÃ”NG TÃŒM THáº¤Y FOLDER CHECKPOINT!\n",
            "   => Model Ä‘ang dÃ¹ng RANDOM WEIGHTS - ChÆ°a train!\n",
            "\n",
            "Model training mode: False\n",
            "\n",
            "Decoder trainable params: 152,806,656 / 152,806,656\n"
          ]
        }
      ]
    }
  ]
}