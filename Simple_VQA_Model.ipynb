{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMC+5toMpd4ED53lAMgVCyy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenminhvuinfo/250505-mern/blob/main/Simple_VQA_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "ocIo5PvN8CHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, DataCollatorWithPadding, AutoModel, AutoModelForCausalLM, AutoImageProcessor\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from datasets import load_dataset, Dataset"
      ],
      "metadata": {
        "id": "GmpCrbzn8-qm"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-uVjiHr69CBM",
        "outputId": "863a49a8-9740-4f99-b187-96424a7968d6"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_ENCODER_MODEL = 'google/vit-base-patch16-224'\n",
        "TEXT_ENCODER_MODEL = 'distilbert/distilbert-base-cased-distilled-squad'\n",
        "DECODER_MODEL = 'gpt2'"
      ],
      "metadata": {
        "id": "hGjhNR8l98ej"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_tokenizer = AutoTokenizer.from_pretrained(DECODER_MODEL)\n",
        "decoder_tokenizer.pad_token = decoder_tokenizer.eos_token\n",
        "decoder_tokenizer.padding_side = \"left\""
      ],
      "metadata": {
        "id": "xCROTeNc-F7R"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building the model"
      ],
      "metadata": {
        "id": "X7ctNTkv8rSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalVQAModel(nn.Module):\n",
        "\n",
        "    def __init__(self, text_encoder_model: str, image_encoder_model: str, decoder_model: str):\n",
        "        super(MultimodalVQAModel, self).__init__()\n",
        "\n",
        "        # Load pre-trained models\n",
        "        self.text_encoder = AutoModel.from_pretrained(text_encoder_model).to(device)\n",
        "        self.image_encoder = AutoModel.from_pretrained(image_encoder_model).to(device)\n",
        "        self.decoder = AutoModelForCausalLM.from_pretrained(\n",
        "            decoder_model,\n",
        "            add_cross_attention=True,\n",
        "            tie_word_embeddings=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Linear layers to project text and image features to the decoder's hidden size\n",
        "        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
        "        self.image_proj = nn.Linear(self.image_encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
        "\n",
        "    def forward(self, input_text, input_image, decoder_input_ids, attention_mask, labels=None):\n",
        "        # Encode text\n",
        "        text_features = self.encode_text(input_text, attention_mask)\n",
        "        # Encode image\n",
        "        image_features = self.encode_image(input_image)\n",
        "\n",
        "        combined_features = (text_features + image_features) / 2\n",
        "\n",
        "        # Decoding\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            labels=labels,\n",
        "            encoder_hidden_states=combined_features.unsqueeze(1),\n",
        "        )\n",
        "        return decoder_outputs\n",
        "\n",
        "    def encode_text(self, input_text, attention_mask):\n",
        "        text_outputs = self.text_encoder(input_text, attention_mask=attention_mask)\n",
        "        text_features = text_outputs.last_hidden_state.mean(dim=1)\n",
        "        return self.text_proj(text_features)\n",
        "\n",
        "    def encode_image(self, input_image):\n",
        "        image_outputs = self.image_encoder(input_image)\n",
        "        image_features = image_outputs.pooler_output\n",
        "        return self.image_proj(image_features)"
      ],
      "metadata": {
        "id": "NKgoUx159Bjo"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Processing"
      ],
      "metadata": {
        "id": "XHW3nKt88rVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HuggingFaceM4/A-OKVQA\")\n",
        "train_ds = dataset['train']\n",
        "val_ds = dataset['validation']\n",
        "test_ds = dataset['test']\n"
      ],
      "metadata": {
        "id": "gpzP6ADt-gQP"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
        "train_ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8jtC1-e_FLU",
        "outputId": "836267d6-da49-4344-f2d1-e04ff4812ed4"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 17056, Val: 1145, Test: 6702\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>,\n",
              " 'question_id': '22MexNkBPpdZGX6sxbxVBH',\n",
              " 'question': 'What is the man by the bags awaiting?',\n",
              " 'choices': ['skateboarder', 'train', 'delivery', 'cab'],\n",
              " 'correct_choice_idx': 3,\n",
              " 'direct_answers': \"['ride', 'ride', 'bus', 'taxi', 'travelling', 'traffic', 'taxi', 'cab', 'cab', 'his ride']\",\n",
              " 'difficult_direct_answer': False,\n",
              " 'rationales': ['A train would not be on the street, he would not have luggage waiting for a delivery, and the skateboarder is there and not paying attention to him so a cab is the only possible answer.',\n",
              "  'He has bags as if he is going someone, and he is on a road waiting for vehicle that can only be moved on the road and is big enough to hold the bags.',\n",
              "  'He looks to be waiting for a paid ride to pick him up.']}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoder_tokenizer = AutoTokenizer.from_pretrained(TEXT_ENCODER_MODEL)\n",
        "image_feature_extractor = AutoImageProcessor.from_pretrained(IMAGE_ENCODER_MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1hoJmPp_IhX",
        "outputId": "1db9716d-7b97-4ed9-8c40-b7eda1e3df5e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def data_collator(batch):\n",
        "    # Text inputs\n",
        "    text_inputs = [sample['question'] for sample in batch]\n",
        "    text_tensors = text_encoder_tokenizer(text_inputs, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Image inputs\n",
        "    images = [sample['image'].convert('RGB') for sample in batch]\n",
        "    image_inputs = image_feature_extractor(images, return_tensors=\"pt\")\n",
        "    image_tensors = image_inputs['pixel_values']\n",
        "\n",
        "    # ✅ FIX: Parse string thành list trước\n",
        "    target_inputs = []\n",
        "    for sample in batch:\n",
        "        # Parse string thành list\n",
        "        answers = ast.literal_eval(sample['direct_answers'])\n",
        "        # Lấy answer đầu tiên\n",
        "        answer = answers[0]\n",
        "        target_inputs.append(f\"<|endoftext|>{answer}<|endoftext|>\")\n",
        "\n",
        "    target_tensors = decoder_tokenizer(target_inputs, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Labels\n",
        "    labels = target_tensors[\"input_ids\"].clone()\n",
        "    labels = torch.where((labels == decoder_tokenizer.pad_token_id), -100, labels)\n",
        "    labels[:, -1] = decoder_tokenizer.eos_token_id\n",
        "\n",
        "    return {\n",
        "        \"input_text\": text_tensors[\"input_ids\"],\n",
        "        \"attention_mask\": text_tensors[\"attention_mask\"],\n",
        "        \"input_image\": image_tensors,\n",
        "        \"decoder_input_ids\": target_tensors[\"input_ids\"],\n",
        "        \"labels\": labels\n",
        "    }"
      ],
      "metadata": {
        "id": "j_a9G-Dq_Ip4"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data_collator với fix mới\n",
        "batch = data_collator([train_ds[0], train_ds[1]])\n",
        "\n",
        "print(\"Sample 0:\")\n",
        "print(f\"  Question: {train_ds[0]['question']}\")\n",
        "print(f\"  Direct answers (raw): {train_ds[0]['direct_answers']}\")\n",
        "print(f\"  Decoder input: {decoder_tokenizer.decode(batch['decoder_input_ids'][0])}\")\n",
        "print(f\"  Labels: {decoder_tokenizer.decode([t for t in batch['labels'][0] if t != -100])}\")\n",
        "\n",
        "print(\"\\nSample 1:\")\n",
        "print(f\"  Question: {train_ds[1]['question']}\")\n",
        "print(f\"  Direct answers (raw): {train_ds[1]['direct_answers']}\")\n",
        "print(f\"  Decoder input: {decoder_tokenizer.decode(batch['decoder_input_ids'][1])}\")\n",
        "print(f\"  Labels: {decoder_tokenizer.decode([t for t in batch['labels'][1] if t != -100])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABFX7m7TQfxX",
        "outputId": "33c90f76-e9e4-45dd-8e8c-b352743da0e7"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0:\n",
            "  Question: What is the man by the bags awaiting?\n",
            "  Direct answers (raw): ['ride', 'ride', 'bus', 'taxi', 'travelling', 'traffic', 'taxi', 'cab', 'cab', 'his ride']\n",
            "  Decoder input: <|endoftext|>ride<|endoftext|>\n",
            "  Labels: ride<|endoftext|>\n",
            "\n",
            "Sample 1:\n",
            "  Question: Where does this man eat pizza?\n",
            "  Direct answers (raw): ['work', 'office', 'work', 'work', 'at work', 'desk', 'at desk', 'office', 'work desk', 'office']\n",
            "  Decoder input: <|endoftext|>work<|endoftext|>\n",
            "  Labels: work<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator([train_ds[0], train_ds[1]])\n",
        "print(\"Input text shape:\", batch['input_text'].shape)\n",
        "print(\"Input image shape:\", batch['input_image'].shape)\n",
        "print(\"Labels shape:\", batch['labels'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSsge91l_hQA",
        "outputId": "b294028c-42e0-4564-8fbc-853ae7d1e4f2"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text shape: torch.Size([2, 11])\n",
            "Input image shape: torch.Size([2, 3, 224, 224])\n",
            "Labels shape: torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the multimodal VQA model\n"
      ],
      "metadata": {
        "id": "myx807PO8rYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./aokvqa_output\",\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_safetensors=False,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=128,\n",
        "    logging_steps=10,\n",
        "    report_to='none',\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=2e-5,\n",
        "    lr_scheduler_type='cosine',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_num_workers=4,\n",
        ")"
      ],
      "metadata": {
        "id": "EhvhGDOS_lS4"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalVQAModel(\n",
        "    TEXT_ENCODER_MODEL,\n",
        "    IMAGE_ENCODER_MODEL,\n",
        "    DECODER_MODEL\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlLR5HeZ_reM",
        "outputId": "7a642616-b2b5-4428-abe5-5d3f71650ce7"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq_COKOS_xA5",
        "outputId": "768a34fc-6761-46d3-a403-ec1c35234549"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 305174784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "MJKZEEyxAYRf"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "AN1B1KkzAaUy",
        "outputId": "403dc3c5-ec72-4365-db81-57aa685d52e0"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1599' max='1599' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1599/1599 11:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.170500</td>\n",
              "      <td>2.912757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.615200</td>\n",
              "      <td>2.739261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.589700</td>\n",
              "      <td>2.697063</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1599, training_loss=3.1480240288043184, metrics={'train_runtime': 694.1198, 'train_samples_per_second': 73.716, 'train_steps_per_second': 2.304, 'total_flos': 0.0, 'train_loss': 3.1480240288043184, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"aokvqa_model.pt\")\n",
        "print(\"✅ Model saved as aokvqa_model.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfwtHmZgDNMU",
        "outputId": "0c6f1e4b-83ae-4641-8116-10c00892e2dd"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved as aokvqa_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST\n"
      ],
      "metadata": {
        "id": "JqSoKhIYDU9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test với sample từ train_ds\n",
        "test_model_train = lambda idx: test_model_from_dataset(idx, train_ds)\n",
        "\n",
        "def test_model_from_dataset(idx, dataset):\n",
        "    sample = dataset[idx]\n",
        "\n",
        "    plt.imshow(sample['image'])\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Q: {sample['question']}\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Ground Truth: {sample['direct_answers']}\")\n",
        "\n",
        "    batch = data_collator([sample])\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(batch[\"input_text\"], batch[\"attention_mask\"])\n",
        "        image_features = model.encode_image(batch[\"input_image\"])\n",
        "        combined_features = (text_features + image_features) / 2\n",
        "\n",
        "        attention_mask = torch.ones((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "        generated_ids = model.decoder.generate(\n",
        "            encoder_hidden_states=combined_features.unsqueeze(1),\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=20,\n",
        "            num_beams=3,\n",
        "            pad_token_id=decoder_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        prediction = decoder_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        print(f\"Prediction: {prediction}\")\n",
        "\n",
        "# Test 10 ví dụ từ tập train\n",
        "for i in range(10):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"EXAMPLE {i+1}\")\n",
        "    print('='*50)\n",
        "    test_model_from_dataset(i, train_ds)"
      ],
      "metadata": {
        "id": "QbVnu2eCDgnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== KIỂM TRA QUAN TRỌNG NHẤT =====\n",
        "print(\"=\"*80)\n",
        "print(\"🔍 KIỂM TRA: Model đã được train chưa?\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Kiểm tra 1: Có file checkpoint không?\n",
        "import os\n",
        "checkpoint_dir = \"checkpoints\"  # Thay bằng folder của bạn\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth') or f.endswith('.pt')]\n",
        "    print(f\"✅ Tìm thấy {len(checkpoints)} checkpoints: {checkpoints}\")\n",
        "else:\n",
        "    print(f\"❌ KHÔNG TÌM THẤY FOLDER CHECKPOINT!\")\n",
        "    print(\"   => Model đang dùng RANDOM WEIGHTS - Chưa train!\")\n",
        "\n",
        "# Kiểm tra 2: Model có ở evaluation mode không?\n",
        "print(f\"\\nModel training mode: {model.training}\")\n",
        "if model.training:\n",
        "    print(\"⚠️ WARNING: Model đang ở TRAINING mode, phải chuyển sang eval!\")\n",
        "    model.eval()\n",
        "\n",
        "# Kiểm tra 3: Decoder có được train không?\n",
        "decoder_trainable_params = sum(p.numel() for p in model.decoder.parameters() if p.requires_grad)\n",
        "decoder_total_params = sum(p.numel() for p in model.decoder.parameters())\n",
        "print(f\"\\nDecoder trainable params: {decoder_trainable_params:,} / {decoder_total_params:,}\")\n",
        "\n",
        "if decoder_trainable_params == 0:\n",
        "    print(\"❌ DECODER BỊ FREEZE - Không thể train được!\")\n",
        "    print(\"   => Phải unfreeze decoder trước khi train\")\n",
        "\n",
        "# ===== NGUYÊN NHÂN: Bạn chưa train model =====\n",
        "# ➡️ GIẢI PHÁP: Train model trước khi test!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcUxhfXrVQMM",
        "outputId": "73423947-2e14-462b-ce39-302a1c67bd34"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "🔍 KIỂM TRA: Model đã được train chưa?\n",
            "================================================================================\n",
            "❌ KHÔNG TÌM THẤY FOLDER CHECKPOINT!\n",
            "   => Model đang dùng RANDOM WEIGHTS - Chưa train!\n",
            "\n",
            "Model training mode: False\n",
            "\n",
            "Decoder trainable params: 152,806,656 / 152,806,656\n"
          ]
        }
      ]
    }
  ]
}