{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/LGH4J1Pow+xsTwutkmrC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenminhvuinfo/250505-mern/blob/main/Simple_VQA_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "ocIo5PvN8CHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, DataCollatorWithPadding, AutoModel, AutoModelForCausalLM, AutoImageProcessor\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from datasets import load_dataset, Dataset"
      ],
      "metadata": {
        "id": "GmpCrbzn8-qm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-uVjiHr69CBM",
        "outputId": "77149784-cc9d-483c-d7ce-479cd90ebb6b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_ENCODER_MODEL = 'google/vit-base-patch16-224'\n",
        "TEXT_ENCODER_MODEL = 'distilbert/distilbert-base-cased-distilled-squad'\n",
        "DECODER_MODEL = 'gpt2'"
      ],
      "metadata": {
        "id": "hGjhNR8l98ej"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_tokenizer = AutoTokenizer.from_pretrained(DECODER_MODEL)\n",
        "decoder_tokenizer.pad_token = decoder_tokenizer.eos_token\n",
        "decoder_tokenizer.padding_side = \"left\""
      ],
      "metadata": {
        "id": "xCROTeNc-F7R"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building the model"
      ],
      "metadata": {
        "id": "X7ctNTkv8rSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalVQAModel(nn.Module):\n",
        "\n",
        "    def __init__(self, text_encoder_model: str, image_encoder_model: str, decoder_model: str):\n",
        "        super(MultimodalVQAModel, self).__init__()\n",
        "\n",
        "        # Load pre-trained models\n",
        "        self.text_encoder = AutoModel.from_pretrained(text_encoder_model).to(device)\n",
        "        self.image_encoder = AutoModel.from_pretrained(image_encoder_model).to(device)\n",
        "        self.decoder = AutoModelForCausalLM.from_pretrained(\n",
        "            decoder_model,\n",
        "            add_cross_attention=True,\n",
        "            tie_word_embeddings=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Linear layers to project text and image features to the decoder's hidden size\n",
        "        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
        "        self.image_proj = nn.Linear(self.image_encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
        "\n",
        "    def forward(self, input_text, input_image, decoder_input_ids, attention_mask, labels=None):\n",
        "        # Encode text\n",
        "        text_features = self.encode_text(input_text, attention_mask)\n",
        "        # Encode image\n",
        "        image_features = self.encode_image(input_image)\n",
        "\n",
        "        combined_features = (text_features + image_features) / 2\n",
        "\n",
        "        # Decoding\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            labels=labels,\n",
        "            encoder_hidden_states=combined_features.unsqueeze(1),\n",
        "        )\n",
        "        return decoder_outputs\n",
        "\n",
        "    def encode_text(self, input_text, attention_mask):\n",
        "        text_outputs = self.text_encoder(input_text, attention_mask=attention_mask)\n",
        "        text_features = text_outputs.last_hidden_state.mean(dim=1)\n",
        "        return self.text_proj(text_features)\n",
        "\n",
        "    def encode_image(self, input_image):\n",
        "        image_outputs = self.image_encoder(input_image)\n",
        "        image_features = image_outputs.pooler_output\n",
        "        return self.image_proj(image_features)"
      ],
      "metadata": {
        "id": "NKgoUx159Bjo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Processing"
      ],
      "metadata": {
        "id": "XHW3nKt88rVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HuggingFaceM4/A-OKVQA\")\n",
        "train_ds = dataset['train']\n",
        "val_ds = dataset['validation']\n",
        "test_ds = dataset['test']\n",
        "\n",
        "# Giảm dataset\n",
        "train_ds = train_ds.select(range(1000))\n",
        "val_ds = val_ds.select(range(200))"
      ],
      "metadata": {
        "id": "gpzP6ADt-gQP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
        "train_ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8jtC1-e_FLU",
        "outputId": "e02b0387-461e-48b7-c7ef-1c44320f9073"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1000, Val: 200, Test: 6702\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>,\n",
              " 'question_id': '22MexNkBPpdZGX6sxbxVBH',\n",
              " 'question': 'What is the man by the bags awaiting?',\n",
              " 'choices': ['skateboarder', 'train', 'delivery', 'cab'],\n",
              " 'correct_choice_idx': 3,\n",
              " 'direct_answers': \"['ride', 'ride', 'bus', 'taxi', 'travelling', 'traffic', 'taxi', 'cab', 'cab', 'his ride']\",\n",
              " 'difficult_direct_answer': False,\n",
              " 'rationales': ['A train would not be on the street, he would not have luggage waiting for a delivery, and the skateboarder is there and not paying attention to him so a cab is the only possible answer.',\n",
              "  'He has bags as if he is going someone, and he is on a road waiting for vehicle that can only be moved on the road and is big enough to hold the bags.',\n",
              "  'He looks to be waiting for a paid ride to pick him up.']}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoder_tokenizer = AutoTokenizer.from_pretrained(TEXT_ENCODER_MODEL)\n",
        "image_feature_extractor = AutoImageProcessor.from_pretrained(IMAGE_ENCODER_MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1hoJmPp_IhX",
        "outputId": "95133a02-6c30-4ae5-dd3e-e854b1db56c1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_collator(batch):\n",
        "    # Text inputs\n",
        "    text_inputs = [sample['question'] for sample in batch]\n",
        "    text_tensors = text_encoder_tokenizer(text_inputs, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Image inputs\n",
        "    images = [sample['image'].convert('RGB') for sample in batch]\n",
        "    image_inputs = image_feature_extractor(images, return_tensors=\"pt\")\n",
        "    image_tensors = image_inputs['pixel_values']\n",
        "\n",
        "    # Target answers (lấy answer đầu tiên)\n",
        "    target_inputs = [f\"<|endoftext|>{sample['direct_answers'][0]}<|endoftext|>\" for sample in batch]\n",
        "    target_tensors = decoder_tokenizer(target_inputs, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Labels\n",
        "    labels = target_tensors[\"input_ids\"].clone()\n",
        "    labels = torch.where((labels == decoder_tokenizer.pad_token_id), -100, labels)\n",
        "    labels[:, -1] = decoder_tokenizer.eos_token_id\n",
        "\n",
        "    return {\n",
        "        \"input_text\": text_tensors[\"input_ids\"],\n",
        "        \"attention_mask\": text_tensors[\"attention_mask\"],\n",
        "        \"input_image\": image_tensors,\n",
        "        \"decoder_input_ids\": target_tensors[\"input_ids\"],\n",
        "        \"labels\": labels\n",
        "    }"
      ],
      "metadata": {
        "id": "j_a9G-Dq_Ip4"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator([train_ds[0], train_ds[1]])\n",
        "print(\"Input text shape:\", batch['input_text'].shape)\n",
        "print(\"Input image shape:\", batch['input_image'].shape)\n",
        "print(\"Labels shape:\", batch['labels'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSsge91l_hQA",
        "outputId": "05f15881-725b-48ad-fa6f-883cb4982c1c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text shape: torch.Size([2, 11])\n",
            "Input image shape: torch.Size([2, 3, 224, 224])\n",
            "Labels shape: torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the multimodal VQA model\n"
      ],
      "metadata": {
        "id": "myx807PO8rYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./aokvqa_output\",\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_safetensors=False,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=128,\n",
        "    logging_steps=10,\n",
        "    report_to='none',\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=2e-5,\n",
        "    lr_scheduler_type='cosine',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_num_workers=4,\n",
        ")"
      ],
      "metadata": {
        "id": "EhvhGDOS_lS4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalVQAModel(\n",
        "    TEXT_ENCODER_MODEL,\n",
        "    IMAGE_ENCODER_MODEL,\n",
        "    DECODER_MODEL\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlLR5HeZ_reM",
        "outputId": "cff17c42-3bf8-4b38-ee97-19895fcc3b5b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq_COKOS_xA5",
        "outputId": "15c8c6fc-70ec-4e1b-b392-cafc57e56d1e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 305174784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "MJKZEEyxAYRf"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "AN1B1KkzAaUy",
        "outputId": "9daef715-213f-4e91-94ce-5b82292bb992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='97' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 00:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.568700</td>\n",
              "      <td>0.000009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.020900</td>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"aokvqa_model.pt\")\n",
        "print(\"✅ Model saved as aokvqa_model.pt\")"
      ],
      "metadata": {
        "id": "xfwtHmZgDNMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST\n"
      ],
      "metadata": {
        "id": "JqSoKhIYDU9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(idx):\n",
        "    sample = val_ds[idx]\n",
        "\n",
        "    plt.imshow(sample['image'])\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Q: {sample['question']}\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Ground Truth: {sample['direct_answers']}\")\n",
        "\n",
        "    batch = data_collator([sample])\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(batch[\"input_text\"], batch[\"attention_mask\"])\n",
        "        image_features = model.encode_image(batch[\"input_image\"])\n",
        "        combined_features = (text_features + image_features) / 2\n",
        "\n",
        "        # Tạo attention_mask cho decoder\n",
        "        attention_mask = torch.ones((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "        generated_ids = model.decoder.generate(\n",
        "            encoder_hidden_states=combined_features.unsqueeze(1),\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=20,\n",
        "            num_beams=3,\n",
        "            pad_token_id=decoder_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        prediction = decoder_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        print(f\"Prediction: {prediction}\")\n",
        "\n",
        "test_model(0)"
      ],
      "metadata": {
        "id": "QbVnu2eCDgnd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}